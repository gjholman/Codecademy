Machine Learning Notes

- Founded by IBM's Arthur Samuel
	- Developed a game that played checkers in 1959
	- The field of study that gives computers the ability to learn without being explicitly programmed
- Concept originally thought of by Alan Turning -> can machines think?
	- From his published Computing Machinery and Intelligence

- Supervised vs. Unsupervised Learning
	- Supervised learns to predict the output from the input data
		- Regression
			- Continuous valued output: Housing Price in New York? Cryptocurrency Value?
		- Classification
			- Predict discrete number of values: Is this a picture of a dog? How many dogs? Is this email spam?
	- Unsupervised Learning learns the inherent structure of a dataset

- Linear Regression
	- Best fit line
	- Convergence: when loss stops changing
- Multiple Linear Regression
	- using multiple data sets to predict the outcome
	- to do this we split the data into training and testing sets using sklearn
	- Then, residual analysis allows us to analyze the model accuracy

- Regression vs Classfication
	- Regression: Continuous data -> 
		- Used to predict:
			- height of plant from rainfall
			- salary based on age and availability of high-speed internet
			- car's mpg based on size and model year
		- Linear regression
			- y=mx+b
		- Logistic regression
			- Probability of 1/0
		- Polynomial regression
			- y= m1x^(n) + m2x^(n-1) + m3x^(n-2) + .... + b
		- Stepwise regression
			- multiple variables: removes and adds predictors for each step
			- backward elimination removes least significant variable from each predictor
		- Ridge Regression
			- not totally sure
		- Lasso regression
			- Least absolute shrinkage selection operator
	- Classification: discrete label
		- Used to predict:
			- Spam email
			- will it rain?
			- user is power user or casual user?
		- multilabel classification
			- multiple possible outcomes, useful for customer segmentation, item gategorization, sentiment analysis (understanding text)
		- Naive Bayes
		- K-nearest neighbors
		- SVMs



- Normalizing Data
	- Split data into typically 80% training and 20% into test
	- Typically this ends up with a lot of variance, so we look at N-Fold Cross-Validation
	- Where N times, we make 1/Nth of the data into the validation and the rest to training.
	- Understanding the accuracy of the model

- K-Nearest-Neighbors
	- In a 2D example, find the k number of points within. Whichever attribute is most common, that is the correct answer
	- Steps:
		- Normalize
		- find k nearest neighbors
		- classify new point based on neighbors
	- Overfitting: rely too heavily on training data. In KNN, not considering enough neighbors
	- Underfitting: inverse of overfitting, KNN considers too many neighbors